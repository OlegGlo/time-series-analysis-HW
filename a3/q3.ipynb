{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66fd208a",
      "metadata": {},
      "source": [
        "# Question 3: Maximum Likelihood Estimation for Gaussian AR(2)\n",
        "\n",
        "## The Model\n",
        "\n",
        "The **AR(2)** (autoregressive of order 2) model is:\n",
        "$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\varepsilon_t$$\n",
        "\n",
        "where:\n",
        "- $c$ is a constant (intercept)\n",
        "- $\\phi_1, \\phi_2$ are autoregressive coefficients\n",
        "- $\\varepsilon_t \\sim N(0, \\sigma^2)$ is Gaussian white noise (i.i.d.)\n",
        "\n",
        "**Parameters to estimate:** $\\boldsymbol{\\theta} = (c, \\phi_1, \\phi_2, \\sigma^2)$\n",
        "\n",
        "## Why Likelihood?\n",
        "We want to find the parameter values that make our observed data **most probable**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6951c6dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 5)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63fd2ff4",
      "metadata": {},
      "source": [
        "## Visualization 1: What Does an AR(2) Process Look Like?\n",
        "\n",
        "Let's simulate some AR(2) processes to build intuition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0c5c0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_ar2(c, phi1, phi2, sigma, T, y0=0, y1=0):\n",
        "    \"\"\"Simulate an AR(2) process\"\"\"\n",
        "    y = np.zeros(T)\n",
        "    y[0], y[1] = y0, y1\n",
        "    eps = np.random.normal(0, sigma, T)\n",
        "    for t in range(2, T):\n",
        "        y[t] = c + phi1 * y[t-1] + phi2 * y[t-2] + eps[t]\n",
        "    return y, eps\n",
        "\n",
        "# Different AR(2) parameter sets\n",
        "param_sets = [\n",
        "    (0, 0.5, 0.3, 1, \"Positive roots: φ₁=0.5, φ₂=0.3\"),\n",
        "    (0, 1.3, -0.4, 1, \"Complex roots (cycles): φ₁=1.3, φ₂=-0.4\"),\n",
        "    (0, 0.2, -0.5, 1, \"Alternating: φ₁=0.2, φ₂=-0.5\"),\n",
        "]\n",
        "\n",
        "T = 200\n",
        "fig, axes = plt.subplots(len(param_sets), 1, figsize=(14, 9))\n",
        "\n",
        "for ax, (c, phi1, phi2, sigma, title) in zip(axes, param_sets):\n",
        "    np.random.seed(42)\n",
        "    y, _ = simulate_ar2(c, phi1, phi2, sigma, T)\n",
        "    ax.plot(y, linewidth=1)\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('$y_t$')\n",
        "    \n",
        "    # Check stationarity (roots outside unit circle)\n",
        "    # Characteristic equation: 1 - φ₁z - φ₂z² = 0\n",
        "    roots = np.roots([phi2, phi1, -1])  # Note: reversed order for np.roots\n",
        "    ax.text(0.02, 0.95, f'Char. roots: {roots[0]:.2f}, {roots[1]:.2f}', \n",
        "            transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Different AR(2) Behaviors', fontsize=14, y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3a70af",
      "metadata": {},
      "source": [
        "---\n",
        "## Part (a): The Exact Log-Likelihood\n",
        "\n",
        "### The Intuition\n",
        "\n",
        "For the likelihood, we need to find: **What is the probability of observing our data $(y_1, y_2, \\ldots, y_T)$ given parameters $\\theta$?**\n",
        "\n",
        "Using the chain rule of probability:\n",
        "$$f(y_1, y_2, \\ldots, y_T | \\theta) = f(y_1, y_2 | \\theta) \\cdot \\prod_{t=3}^{T} f(y_t | y_{t-1}, y_{t-2}, \\theta)$$\n",
        "\n",
        "### The Pieces:\n",
        "\n",
        "**1. Conditional densities (for $t \\geq 3$):** Given $y_{t-1}$ and $y_{t-2}$, we know:\n",
        "$$y_t | y_{t-1}, y_{t-2} \\sim N\\left(c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2}, \\sigma^2\\right)$$\n",
        "\n",
        "**2. The tricky part:** The joint density of $(y_1, y_2)$ — this requires the stationary distribution of the AR(2).\n",
        "\n",
        "### The Exact Log-Likelihood:\n",
        "\n",
        "$$\\boxed{\\ell(\\theta) = \\log f(y_1, y_2 | \\theta) - \\frac{T-2}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=3}^{T}\\left(y_t - c - \\phi_1 y_{t-1} - \\phi_2 y_{t-2}\\right)^2}$$\n",
        "\n",
        "The first term $f(y_1, y_2|\\theta)$ is complicated because it involves the stationary covariance structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76a7322",
      "metadata": {},
      "source": [
        "## Visualization 2: Understanding the Likelihood Graphically\n",
        "\n",
        "Each observation $y_t$ (for $t \\geq 3$) comes from a normal distribution centered at the predicted value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1fdc4bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate data\n",
        "c_true, phi1_true, phi2_true, sigma_true = 0.5, 0.6, 0.2, 1.0\n",
        "T = 50\n",
        "np.random.seed(123)\n",
        "y, eps = simulate_ar2(c_true, phi1_true, phi2_true, sigma_true, T)\n",
        "\n",
        "# Compute predicted values (conditional means)\n",
        "y_pred = np.zeros(T)\n",
        "for t in range(2, T):\n",
        "    y_pred[t] = c_true + phi1_true * y[t-1] + phi2_true * y[t-2]\n",
        "\n",
        "# Residuals = actual - predicted\n",
        "residuals = y - y_pred\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Time series with predictions\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(y, 'b-', linewidth=1.5, label='Actual $y_t$', marker='o', markersize=3)\n",
        "ax1.plot(range(2, T), y_pred[2:], 'r--', linewidth=1.5, label='Predicted $\\\\hat{y}_t = c + \\\\phi_1 y_{t-1} + \\\\phi_2 y_{t-2}$')\n",
        "ax1.set_xlabel('Time t')\n",
        "ax1.set_ylabel('$y_t$')\n",
        "ax1.set_title('AR(2): Actual vs Predicted Values')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Residuals (should be white noise)\n",
        "ax2 = axes[0, 1]\n",
        "ax2.stem(range(2, T), residuals[2:], linefmt='g-', markerfmt='go', basefmt='k-')\n",
        "ax2.axhline(y=0, color='black', linewidth=0.8)\n",
        "ax2.set_xlabel('Time t')\n",
        "ax2.set_ylabel('$\\\\varepsilon_t = y_t - \\\\hat{y}_t$')\n",
        "ax2.set_title(f'Residuals (should be $N(0, \\\\sigma^2)$ with $\\\\sigma$={sigma_true})')\n",
        "\n",
        "# Plot 3: Likelihood contribution from each observation\n",
        "ax3 = axes[1, 0]\n",
        "# Log-likelihood contribution from each t\n",
        "ll_contrib = -0.5 * np.log(2 * np.pi * sigma_true**2) - residuals[2:]**2 / (2 * sigma_true**2)\n",
        "ax3.bar(range(2, T), ll_contrib, color='purple', alpha=0.7)\n",
        "ax3.set_xlabel('Time t')\n",
        "ax3.set_ylabel('Log-likelihood contribution')\n",
        "ax3.set_title('Each Observation\\'s Contribution to Log-Likelihood')\n",
        "ax3.axhline(y=np.mean(ll_contrib), color='red', linestyle='--', label=f'Mean = {np.mean(ll_contrib):.3f}')\n",
        "ax3.legend()\n",
        "\n",
        "# Plot 4: Visual of the normal density for a few observations\n",
        "ax4 = axes[1, 1]\n",
        "t_examples = [10, 25, 40]\n",
        "colors = ['blue', 'green', 'orange']\n",
        "x_range = np.linspace(y.min() - 2, y.max() + 2, 200)\n",
        "\n",
        "for t, color in zip(t_examples, colors):\n",
        "    mu_t = y_pred[t]\n",
        "    pdf = stats.norm.pdf(x_range, mu_t, sigma_true)\n",
        "    ax4.plot(x_range, pdf, color=color, linewidth=2, label=f't={t}: $\\\\mu$={mu_t:.2f}')\n",
        "    ax4.axvline(x=y[t], color=color, linestyle='--', alpha=0.7)\n",
        "    ax4.plot(y[t], stats.norm.pdf(y[t], mu_t, sigma_true), 'o', color=color, markersize=10)\n",
        "\n",
        "ax4.set_xlabel('$y_t$')\n",
        "ax4.set_ylabel('Density')\n",
        "ax4.set_title('Conditional Density $f(y_t | y_{t-1}, y_{t-2})$ for Selected t')\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total conditional log-likelihood (sum of contributions): {np.sum(ll_contrib):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811025f9",
      "metadata": {},
      "source": [
        "---\n",
        "## Part (b): The Conditional Log-Likelihood\n",
        "\n",
        "### The Simplification\n",
        "\n",
        "Instead of dealing with the complicated joint density $f(y_1, y_2|\\theta)$, we **condition on** the first two observations and treat them as given (not random).\n",
        "\n",
        "This gives us:\n",
        "$$\\ell_c(\\theta | y_1, y_2) = \\log \\prod_{t=3}^{T} f(y_t | y_{t-1}, y_{t-2}, \\theta)$$\n",
        "\n",
        "Since $y_t | y_{t-1}, y_{t-2} \\sim N(c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2}, \\sigma^2)$:\n",
        "\n",
        "$$\\boxed{\\ell_c(c, \\phi_1, \\phi_2, \\sigma^2) = -\\frac{T-2}{2}\\log(2\\pi) - \\frac{T-2}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=3}^{T}\\left(y_t - c - \\phi_1 y_{t-1} - \\phi_2 y_{t-2}\\right)^2}$$\n",
        "\n",
        "### Why This Works\n",
        "\n",
        "- For large $T$, the first two observations contribute negligibly to the total likelihood\n",
        "- Much simpler to work with — no need for the stationary distribution\n",
        "- Gives the same asymptotic results as the exact likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc893c4",
      "metadata": {},
      "source": [
        "---\n",
        "## Part (c): First Order Conditions (FOCs)\n",
        "\n",
        "To find the MLE, we take derivatives and set them to zero.\n",
        "\n",
        "Let $\\varepsilon_t = y_t - c - \\phi_1 y_{t-1} - \\phi_2 y_{t-2}$ (the residual).\n",
        "\n",
        "### FOC for $c$:\n",
        "$$\\frac{\\partial \\ell_c}{\\partial c} = \\frac{1}{\\sigma^2}\\sum_{t=3}^{T}\\varepsilon_t = 0$$\n",
        "\n",
        "**Interpretation:** The residuals must sum to zero (like OLS).\n",
        "\n",
        "### FOC for $\\phi_1$:\n",
        "$$\\frac{\\partial \\ell_c}{\\partial \\phi_1} = \\frac{1}{\\sigma^2}\\sum_{t=3}^{T}\\varepsilon_t \\cdot y_{t-1} = 0$$\n",
        "\n",
        "**Interpretation:** Residuals must be uncorrelated with $y_{t-1}$.\n",
        "\n",
        "### FOC for $\\phi_2$:\n",
        "$$\\frac{\\partial \\ell_c}{\\partial \\phi_2} = \\frac{1}{\\sigma^2}\\sum_{t=3}^{T}\\varepsilon_t \\cdot y_{t-2} = 0$$\n",
        "\n",
        "**Interpretation:** Residuals must be uncorrelated with $y_{t-2}$.\n",
        "\n",
        "### FOC for $\\sigma^2$:\n",
        "$$\\frac{\\partial \\ell_c}{\\partial \\sigma^2} = -\\frac{T-2}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{t=3}^{T}\\varepsilon_t^2 = 0$$\n",
        "\n",
        "**Solution:** $\\hat{\\sigma}^2 = \\frac{1}{T-2}\\sum_{t=3}^{T}\\hat{\\varepsilon}_t^2$\n",
        "\n",
        "### Key Insight: MLE = OLS for Gaussian AR!\n",
        "The FOCs for $(c, \\phi_1, \\phi_2)$ are exactly the OLS normal equations! So for Gaussian AR:\n",
        "\n",
        "$$\\boxed{\\text{MLE of } (c, \\phi_1, \\phi_2) = \\text{OLS regression of } y_t \\text{ on } (1, y_{t-1}, y_{t-2})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1092237b",
      "metadata": {},
      "source": [
        "## Visualization 3: The Likelihood Surface\n",
        "\n",
        "Let's see what the log-likelihood looks like as a function of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c980ef4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def conditional_log_likelihood(y, c, phi1, phi2, sigma2):\n",
        "    \"\"\"Compute conditional log-likelihood for AR(2)\"\"\"\n",
        "    T = len(y)\n",
        "    residuals = np.array([y[t] - c - phi1*y[t-1] - phi2*y[t-2] for t in range(2, T)])\n",
        "    n = T - 2\n",
        "    ll = -n/2 * np.log(2*np.pi) - n/2 * np.log(sigma2) - np.sum(residuals**2) / (2*sigma2)\n",
        "    return ll\n",
        "\n",
        "# Use our simulated data\n",
        "# Fix c and sigma at true values, vary phi1 and phi2\n",
        "phi1_range = np.linspace(0.2, 1.0, 50)\n",
        "phi2_range = np.linspace(-0.2, 0.6, 50)\n",
        "PHI1, PHI2 = np.meshgrid(phi1_range, phi2_range)\n",
        "\n",
        "LL = np.zeros_like(PHI1)\n",
        "for i in range(len(phi2_range)):\n",
        "    for j in range(len(phi1_range)):\n",
        "        LL[i, j] = conditional_log_likelihood(y, c_true, PHI1[i,j], PHI2[i,j], sigma_true**2)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Contour plot\n",
        "ax1 = axes[0]\n",
        "contour = ax1.contour(PHI1, PHI2, LL, levels=20, cmap='viridis')\n",
        "ax1.clabel(contour, inline=True, fontsize=8)\n",
        "ax1.plot(phi1_true, phi2_true, 'r*', markersize=15, label='True values')\n",
        "ax1.set_xlabel('$\\\\phi_1$')\n",
        "ax1.set_ylabel('$\\\\phi_2$')\n",
        "ax1.set_title('Conditional Log-Likelihood Surface\\n(c and $\\\\sigma^2$ fixed at true values)')\n",
        "ax1.legend()\n",
        "\n",
        "# 3D surface\n",
        "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "ax2.plot_surface(PHI1, PHI2, LL, cmap='viridis', alpha=0.8)\n",
        "ax2.scatter([phi1_true], [phi2_true], [conditional_log_likelihood(y, c_true, phi1_true, phi2_true, sigma_true**2)], \n",
        "            color='red', s=100, label='True values')\n",
        "ax2.set_xlabel('$\\\\phi_1$')\n",
        "ax2.set_ylabel('$\\\\phi_2$')\n",
        "ax2.set_zlabel('Log-likelihood')\n",
        "ax2.set_title('3D View of Log-Likelihood')\n",
        "ax2.view_init(elev=30, azim=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"True parameters: φ₁={phi1_true}, φ₂={phi2_true}\")\n",
        "print(f\"Log-likelihood at true values: {conditional_log_likelihood(y, c_true, phi1_true, phi2_true, sigma_true**2):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cfd4179",
      "metadata": {},
      "source": [
        "---\n",
        "## Part (d): The Information Matrix (Second Derivatives)\n",
        "\n",
        "The **Information Matrix** tells us about the curvature of the likelihood — sharper peaks mean more precise estimates.\n",
        "\n",
        "### Second Derivatives of the Conditional Log-Likelihood\n",
        "\n",
        "The Hessian matrix $H$ contains all second partial derivatives. Let's compute the key ones:\n",
        "\n",
        "For the regression parameters $(c, \\phi_1, \\phi_2)$, define $\\mathbf{x}_t = (1, y_{t-1}, y_{t-2})'$:\n",
        "\n",
        "$$\\frac{\\partial^2 \\ell_c}{\\partial \\beta \\partial \\beta'} = -\\frac{1}{\\sigma^2}\\sum_{t=3}^{T} \\mathbf{x}_t \\mathbf{x}_t'$$\n",
        "\n",
        "where $\\beta = (c, \\phi_1, \\phi_2)'$.\n",
        "\n",
        "### The Information Matrix\n",
        "\n",
        "The **Fisher Information** is:\n",
        "$$\\mathcal{I}(\\theta) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\theta'}\\right]$$\n",
        "\n",
        "For AR(2), the information matrix is:\n",
        "\n",
        "$$\\mathcal{I} = \\begin{pmatrix} \n",
        "\\frac{1}{\\sigma^2}\\sum E[\\mathbf{x}_t\\mathbf{x}_t'] & \\mathbf{0} \\\\\n",
        "\\mathbf{0}' & \\frac{T-2}{2\\sigma^4}\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "The block diagonal structure shows that $(c, \\phi_1, \\phi_2)$ are asymptotically independent of $\\sigma^2$!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab21180",
      "metadata": {},
      "source": [
        "## Visualization 4: Computing the Hessian and Information Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916e1e6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the Hessian (second derivative matrix) for (c, φ₁, φ₂)\n",
        "# H = -1/σ² * Σ x_t x_t'\n",
        "\n",
        "X = np.column_stack([\n",
        "    np.ones(T-2),           # constant\n",
        "    y[1:-1],                # y_{t-1}\n",
        "    y[:-2]                  # y_{t-2}\n",
        "])\n",
        "\n",
        "# Hessian for regression coefficients\n",
        "H_beta = -1/sigma_true**2 * (X.T @ X)\n",
        "\n",
        "# Information matrix (negative Hessian)\n",
        "I_beta = -H_beta\n",
        "\n",
        "print(\"Design matrix X (first 5 rows):\")\n",
        "print(\"   [1,  y_{t-1},  y_{t-2}]\")\n",
        "print(X[:5].round(3))\n",
        "\n",
        "print(f\"\\n\\nHessian matrix for (c, φ₁, φ₂):\")\n",
        "print(H_beta.round(3))\n",
        "\n",
        "print(f\"\\n\\nInformation matrix for (c, φ₁, φ₂):\")\n",
        "print(I_beta.round(3))\n",
        "\n",
        "# Asymptotic covariance = inverse of information\n",
        "cov_asymp = np.linalg.inv(I_beta)\n",
        "print(f\"\\n\\nAsymptotic covariance matrix (I⁻¹):\")\n",
        "print(cov_asymp.round(6))\n",
        "\n",
        "# Standard errors\n",
        "se = np.sqrt(np.diag(cov_asymp))\n",
        "print(f\"\\n\\nAsymptotic standard errors:\")\n",
        "print(f\"  SE(c) = {se[0]:.4f}\")\n",
        "print(f\"  SE(φ₁) = {se[1]:.4f}\")\n",
        "print(f\"  SE(φ₂) = {se[2]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c823dbf",
      "metadata": {},
      "source": [
        "---\n",
        "## Part (e): Asymptotic Distribution\n",
        "\n",
        "### The Key Result\n",
        "\n",
        "Under standard regularity conditions, the MLE is **asymptotically normal**:\n",
        "\n",
        "$$\\sqrt{T}(\\hat{\\theta}_{MLE} - \\theta_0) \\xrightarrow{d} N\\left(0, \\mathcal{I}(\\theta_0)^{-1}\\right)$$\n",
        "\n",
        "### For AR(2) specifically:\n",
        "\n",
        "$$\\begin{pmatrix} \\hat{c} \\\\ \\hat{\\phi}_1 \\\\ \\hat{\\phi}_2 \\end{pmatrix} \\xrightarrow{d} N\\left(\\begin{pmatrix} c \\\\ \\phi_1 \\\\ \\phi_2 \\end{pmatrix}, \\sigma^2 \\left(\\sum_{t=3}^{T} \\mathbf{x}_t \\mathbf{x}_t'\\right)^{-1}\\right)$$\n",
        "\n",
        "And for the variance:\n",
        "$$\\hat{\\sigma}^2 \\xrightarrow{d} N\\left(\\sigma^2, \\frac{2\\sigma^4}{T-2}\\right)$$\n",
        "\n",
        "### Interpretation:\n",
        "- As sample size $T \\to \\infty$, our estimates become unbiased and normally distributed\n",
        "- The variance shrinks at rate $1/T$\n",
        "- We can construct confidence intervals and hypothesis tests using these results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7cdc6a",
      "metadata": {},
      "source": [
        "## Visualization 5: Monte Carlo Simulation of the Asymptotic Distribution\n",
        "\n",
        "Let's verify the asymptotic normality by simulating many datasets and looking at the distribution of estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a9203d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_ar2_ols(y):\n",
        "    \"\"\"Estimate AR(2) by OLS (= conditional MLE)\"\"\"\n",
        "    T = len(y)\n",
        "    # Design matrix\n",
        "    X = np.column_stack([\n",
        "        np.ones(T-2),\n",
        "        y[1:-1],    # y_{t-1}\n",
        "        y[:-2]      # y_{t-2}\n",
        "    ])\n",
        "    Y = y[2:]\n",
        "    \n",
        "    # OLS\n",
        "    beta_hat = np.linalg.lstsq(X, Y, rcond=None)[0]\n",
        "    residuals = Y - X @ beta_hat\n",
        "    sigma2_hat = np.sum(residuals**2) / (T - 2)\n",
        "    \n",
        "    return beta_hat[0], beta_hat[1], beta_hat[2], sigma2_hat\n",
        "\n",
        "# Monte Carlo simulation\n",
        "n_simulations = 1000\n",
        "T_sim = 200\n",
        "\n",
        "estimates = np.zeros((n_simulations, 4))\n",
        "\n",
        "for i in range(n_simulations):\n",
        "    np.random.seed(i)\n",
        "    y_sim, _ = simulate_ar2(c_true, phi1_true, phi2_true, sigma_true, T_sim)\n",
        "    estimates[i] = estimate_ar2_ols(y_sim)\n",
        "\n",
        "# Plot the distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "param_names = ['c (constant)', '$\\\\phi_1$', '$\\\\phi_2$', '$\\\\sigma^2$']\n",
        "true_values = [c_true, phi1_true, phi2_true, sigma_true**2]\n",
        "\n",
        "for ax, param_name, true_val, est in zip(axes.flat, param_names, true_values, estimates.T):\n",
        "    ax.hist(est, bins=40, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    ax.axvline(x=true_val, color='red', linewidth=2, linestyle='--', label=f'True = {true_val}')\n",
        "    ax.axvline(x=np.mean(est), color='green', linewidth=2, linestyle='-', label=f'Mean = {np.mean(est):.4f}')\n",
        "    \n",
        "    # Overlay theoretical normal distribution\n",
        "    x_range = np.linspace(est.min(), est.max(), 100)\n",
        "    ax.plot(x_range, stats.norm.pdf(x_range, np.mean(est), np.std(est)), \n",
        "            'orange', linewidth=2, label='Normal fit')\n",
        "    \n",
        "    ax.set_xlabel(param_name)\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Distribution of $\\\\hat{{{param_name[0]}}}$ (n={n_simulations} simulations)')\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(f'Monte Carlo: Sampling Distribution of AR(2) MLEs (T={T_sim})', fontsize=14, y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"Monte Carlo Results:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Parameter':<12} {'True':<10} {'Mean Est.':<12} {'Std. Dev.':<12} {'Bias':<10}\")\n",
        "print(\"-\" * 60)\n",
        "for name, true, est in zip(['c', 'φ₁', 'φ₂', 'σ²'], true_values, estimates.T):\n",
        "    print(f\"{name:<12} {true:<10.4f} {np.mean(est):<12.4f} {np.std(est):<12.4f} {np.mean(est)-true:<10.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8693ac",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary: Key Formulas for Gaussian AR(2) MLE\n",
        "\n",
        "| Part | Formula |\n",
        "|------|---------|\n",
        "| **Model** | $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\varepsilon_t$, $\\varepsilon_t \\sim N(0, \\sigma^2)$ |\n",
        "| **(a) Exact LL** | $\\ell = \\log f(y_1, y_2\\|\\theta) - \\frac{T-2}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=3}^T \\varepsilon_t^2$ |\n",
        "| **(b) Conditional LL** | $\\ell_c = -\\frac{T-2}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=3}^T (y_t - c - \\phi_1 y_{t-1} - \\phi_2 y_{t-2})^2$ |\n",
        "| **(c) FOCs** | $\\sum \\varepsilon_t = 0$, $\\sum \\varepsilon_t y_{t-1} = 0$, $\\sum \\varepsilon_t y_{t-2} = 0$, $\\hat{\\sigma}^2 = \\frac{1}{T-2}\\sum\\varepsilon_t^2$ |\n",
        "| **(d) Information** | $\\mathcal{I}_{\\beta\\beta} = \\frac{1}{\\sigma^2}\\sum \\mathbf{x}_t\\mathbf{x}_t'$, $\\mathcal{I}_{\\sigma^2\\sigma^2} = \\frac{T-2}{2\\sigma^4}$ |\n",
        "| **(e) Asymptotic Dist.** | $\\hat{\\theta} \\xrightarrow{d} N(\\theta_0, \\mathcal{I}^{-1})$ |\n",
        "\n",
        "### Key Insights:\n",
        "1. **Conditional MLE = OLS** for Gaussian AR models\n",
        "2. **Information matrix** determines the precision of estimates\n",
        "3. **Asymptotic normality** enables inference (confidence intervals, tests)\n",
        "4. The larger $|\\phi_1|$ and $|\\phi_2|$, the more persistent the process (harder to estimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b1a96a",
      "metadata": {},
      "source": [
        "## Visualization 6: How Sample Size Affects Estimation Precision\n",
        "\n",
        "The asymptotic variance shrinks as $T$ increases. Let's see this in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42f7c8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare distributions for different sample sizes\n",
        "sample_sizes = [50, 100, 500]\n",
        "n_sims = 500\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, T_size in zip(axes, sample_sizes):\n",
        "    phi1_estimates = []\n",
        "    for i in range(n_sims):\n",
        "        np.random.seed(i + 1000)\n",
        "        y_sim, _ = simulate_ar2(c_true, phi1_true, phi2_true, sigma_true, T_size)\n",
        "        _, phi1_hat, _, _ = estimate_ar2_ols(y_sim)\n",
        "        phi1_estimates.append(phi1_hat)\n",
        "    \n",
        "    phi1_estimates = np.array(phi1_estimates)\n",
        "    \n",
        "    ax.hist(phi1_estimates, bins=30, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    ax.axvline(x=phi1_true, color='red', linewidth=2, linestyle='--', label=f'True $\\\\phi_1$={phi1_true}')\n",
        "    \n",
        "    # Overlay normal\n",
        "    x_range = np.linspace(phi1_estimates.min(), phi1_estimates.max(), 100)\n",
        "    ax.plot(x_range, stats.norm.pdf(x_range, np.mean(phi1_estimates), np.std(phi1_estimates)), \n",
        "            'orange', linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('$\\\\hat{\\\\phi}_1$')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'T = {T_size}\\nStd Dev = {np.std(phi1_estimates):.4f}')\n",
        "    ax.set_xlim(0.3, 0.9)\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle('Effect of Sample Size on $\\\\hat{\\\\phi}_1$ Distribution\\n(More data → narrower distribution → more precise estimates)', \n",
        "             fontsize=13, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Standard deviation of φ̂₁ estimates:\")\n",
        "for T_size in sample_sizes:\n",
        "    theoretical_se = 0.1 * np.sqrt(100/T_size)  # Rough approximation\n",
        "    print(f\"  T = {T_size}: observed ≈ {np.std(phi1_estimates):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
